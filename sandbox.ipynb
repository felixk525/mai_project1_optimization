{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP9as0SOE31z",
        "outputId": "5f54eb85-cb6e-4de5-ca1b-5dce15e82aa1"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !rm -rf *\n",
        "    !git clone --branch multi-thread-optim https://github.com/felixk525/mai_project1_optimization.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IUVrsIRE2l1",
        "outputId": "130245a8-6e6c-45a5-9962-b4b4cea398c1"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !pip3 install -r mai_project1_optimization/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wkD57Ur6E2l2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib created a temporary cache directory at C:\\Users\\krahf\\AppData\\Local\\Temp\\matplotlib-jle7cujr because the default path (C:\\Users\\krahf\\.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.models import *\n",
        "from plotly import express as px\n",
        "from collections import Counter\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import io\n",
        "\n",
        "if(IN_COLAB):\n",
        "    from mai_project1_optimization.modules.dataset import IntelImageClassificationDataset\n",
        "    from mai_project1_optimization.modules.utility import NotebookPlotter, InferenceSession, Evaluator, ISO_time\n",
        "    from mai_project1_optimization.modules.trainer import Trainer\n",
        "else:\n",
        "    from modules.dataset import IntelImageClassificationDataset\n",
        "    from modules.utility import NotebookPlotter, InferenceSession, Evaluator, ISO_time\n",
        "    from modules.trainer import Trainer\n",
        "\n",
        "torch.manual_seed(1)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "def set_seed(seed=1):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True  # for reproducibility\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Removed support for Tensor Units\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "set_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbHgev5PE2l3"
      },
      "source": [
        "https://www.kaggle.com/datasets/puneet6060/intel-image-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3go5RaM8E2l3"
      },
      "outputs": [],
      "source": [
        "# labels, values = zip(*Counter([item[1] for item in dataset.train_dataset]).items())\n",
        "# fig = px.bar(x=labels, y=values, labels={'x': 'Categories', 'y': 'Counts'}, title='Distribution of Classes')\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZypu8jE2l3"
      },
      "source": [
        "| n | label |\n",
        "| --- | --- |\n",
        "| 0 | buildings |\n",
        "| 1 | forest |\n",
        "| 2 | glacier |\n",
        "| 3 | mountain |\n",
        "| 4 | sea |\n",
        "| 5 | street |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3b5bfaa3831e464a930f89f547a25fd6"
          ]
        },
        "id": "5EJgLtsTE2l4",
        "outputId": "0cf0b3eb-2066-4448-cbc1-ed73f35e5424"
      },
      "source": [
        "NotebookPlotter.plot_dataset_item_interactive(dataset.train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "freezeLayer = False\n",
        "\n",
        "dataset = IntelImageClassificationDataset(resize=(150,150))\n",
        "    \n",
        "# SqueezeNet 1.1\n",
        "model = models.squeezenet1_1(weights=SqueezeNet1_1_Weights.DEFAULT)\n",
        "num_features = model.classifier[1].in_channels\n",
        "kernel_size = model.classifier[1].kernel_size\n",
        "if(freezeLayer):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "model.classifier[1] = nn.Conv2d(num_features, 6, kernel_size)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW3sIV9KE2l4",
        "outputId": "8272b2b3-2f5a-46a9-92e4-b0f2548a07c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of CPU cores: 12, using 2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb840100a7a64bbca6690813c6e3248b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 2 workers took 75.73 seconds.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59db7d32f60d4799be6a58df2d6c3048",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with single-threaded loading took 156.77 seconds.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "594c4af79f2f4725a8b3a54850eea16f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 6 workers took 88.90 seconds.\n"
          ]
        }
      ],
      "source": [
        "# model.load_state_dict(torch.load(f\"checkpoints/.pt\"))\n",
        "possible_workers = multiprocessing.cpu_count()\n",
        "print(f\"Number of CPU cores: {possible_workers}, using 2\") # Due to tests - overhead or I/O / dataset to small for it to matter maybe\n",
        "max_worker = possible_workers // 2\n",
        "possible_workers = 2 #possible_workers // 2\n",
        "\n",
        "\n",
        "profiler_config_1 = {\n",
        "    \"schedule\": torch.profiler.schedule(wait=5, warmup=5, active=10, repeat=1),\n",
        "    \"activities\": [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    \"log_dir\": \"runs/profilermthread/2process\",#torch.profiler.tensorboard_trace_handler(\"runs/profilermthread/3process\"),\n",
        "    \"record_shapes\": True,\n",
        "    \"profile_memory\": True,\n",
        "    \"with_stack\": True\n",
        "}\n",
        "\n",
        "profiler_config_2 = {\n",
        "    \"schedule\": torch.profiler.schedule(wait=5, warmup=5, active=10, repeat=1),\n",
        "    \"activities\": [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    \"log_dir\": \"runs/profilermthread/1process\",#torch.profiler.tensorboard_trace_handler(\"runs/profilermthread/1process\"),\n",
        "    \"record_shapes\": True,\n",
        "    \"profile_memory\": True,\n",
        "    \"with_stack\": True\n",
        "}\n",
        "\n",
        "profiler_config_3 = {\n",
        "    \"schedule\": torch.profiler.schedule(wait=5, warmup=5, active=10, repeat=1),\n",
        "    \"activities\": [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    \"log_dir\": \"runs/profilermthread/nprocess\",#torch.profiler.tensorboard_trace_handler(\"runs/profilermthread/1process\"),\n",
        "    \"record_shapes\": True,\n",
        "    \"profile_memory\": True,\n",
        "    \"with_stack\": True\n",
        "}\n",
        "\n",
        "if __name__ == '__main__': # Not necessary for this anymore\n",
        "    pmodel = model\n",
        "    pmodel2 = model\n",
        "    \n",
        "    dataloader = DataLoader(dataset.train_dataset, batch_size=24, shuffle=True, num_workers=possible_workers, pin_memory=True, prefetch_factor= 4, persistent_workers=True)\n",
        "    trainer = Trainer(model=pmodel, lr=0.001)\n",
        "    start_time = time.time()\n",
        "    trainer.train(dataloader, epochs=10, profiler_config=profiler_config_1)\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"Training with {possible_workers} workers took {duration:.2f} seconds.\\n\")\n",
        "\n",
        "    dataloader = DataLoader(dataset.train_dataset, batch_size=24, shuffle=True, pin_memory=True)\n",
        "    trainer = Trainer(model=model, lr=0.001)\n",
        "    start_time = time.time()\n",
        "    trainer.train(dataloader, epochs=10, profiler_config=profiler_config_2)\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"Training with single-threaded loading took {duration:.2f} seconds.\")\n",
        "\n",
        "    if max_worker > 2:\n",
        "        dataloader = DataLoader(dataset.train_dataset, batch_size=24, shuffle=True, num_workers=max_worker, pin_memory=True, prefetch_factor= 4, persistent_workers=True)\n",
        "        trainer = Trainer(model=pmodel2, lr=0.001)\n",
        "        start_time = time.time()\n",
        "        trainer.train(dataloader, epochs=10, profiler_config=profiler_config_3)\n",
        "        duration = time.time() - start_time\n",
        "        print(f\"Training with {max_worker} workers took {duration:.2f} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VESPNHBME2l4",
        "outputId": "d4208f97-8a38-4416-d324-a4027f61ba8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8858695030212402"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "session = InferenceSession(model)\n",
        "output = session(torch.stack(tuple(item[0] for item in dataset.test_dataset)))\n",
        "Evaluator.acc(output, torch.tensor(tuple(item[1] for item in dataset.test_dataset))).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AO9K7B_yE2l5"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), f\"checkpoints/{model.__class__.__name__}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3_7nC3JE2l5"
      },
      "source": [
        "## Initial Results for Model Selection\n",
        "\n",
        "| model | accuracy | size |\n",
        "| --- | --- | --- |\n",
        "| ResNet18 | 0.87 | 44.7 MB |\n",
        "| ResNet34 | 0.88 | 83.3 MB |\n",
        "| MobileNet V2 | 0.91 | 13.6 MB |\n",
        "| MobileNet V3 small | 0.90 | 9.8 MB |\n",
        "| VGG19 | 0.83 | 548.1 MB |\n",
        "| SqueezeNet 1.0 | 0.89 | 4.8 MB |\n",
        "| DenseNet | 0.90 | 30.8 MB |\n",
        "| EfficientNet B0 | 0.92 | 20.5 MB |\n",
        "| ViT-b/16 | 0.73 | 330.3 MB |"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
